from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import pandas as pd
import re
from kiwipiepy import Kiwi

# -----------------------------
# í† í°í™” í•¨ìˆ˜
# -----------------------------
def simple_tokenize(text):
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", "", str(text).lower())
    tokens = text.split()
    return [t for t in tokens if len(t) > 1]

kiwi = Kiwi()

def tokenize_ko(s: str):
    return [
        t.form for t in kiwi.tokenize(s, normalize_coda=True)
        if t.tag.startswith(("N", "V", "VA", "XR", "SL")) and len(t.form) > 1
    ]

# -----------------------------
# 1. data.csv ë¶ˆëŸ¬ì˜¤ê¸°
# -----------------------------
df = pd.read_csv("data/data.csv")  # í•™ìŠµìš© ë¬¸ì¥
documents = [
    TaggedDocument(words=simple_tokenize(row["content"]), tags=[f"csv_{row['id']}"])
    for _, row in df.iterrows()
]

print(f"âœ… data.csv ë¬¸ì„œ ìˆ˜: {len(documents)}")

# -----------------------------
# 2. merged_sample.txt ë¶ˆëŸ¬ì˜¤ê¸°
# -----------------------------
path = "data/raw/merged_sample.txt"
new_docs = [line.strip() for line in open(path, encoding="utf-8") if line.strip()]
tagged_new = [
    TaggedDocument(words=tokenize_ko(s), tags=[f"sample_{i}"])
    for i, s in enumerate(new_docs)
]

print(f"âœ… merged_sample.txt ë¬¸ì„œ ìˆ˜: {len(tagged_new)}")

# -----------------------------
# 3. ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸°
# -----------------------------
all_docs = documents + tagged_new
print(f"ì´ í•™ìŠµ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")

# -----------------------------
# 4. Doc2Vec ëª¨ë¸ í•™ìŠµ
# -----------------------------
model = Doc2Vec(
    vector_size=50,   # ì„ë² ë”© ì°¨ì› ìˆ˜
    window=5,
    min_count=1,
    workers=4,
    epochs=40
)

model.build_vocab(all_docs)
model.train(all_docs, total_examples=len(all_docs), epochs=model.epochs)

# -----------------------------
# 5. ëª¨ë¸ ì €ì¥
# -----------------------------
model.save("models/memo_doc2vec_v3.model")
print("ğŸ‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! â†’ models/memo_doc2vec_v3.model")